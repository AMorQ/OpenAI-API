{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebe2e9d-685d-4ea3-ab7c-9589caaabff5",
   "metadata": {},
   "source": [
    "# Guidelines for Prompting\n",
    "This program is built following the guidelines of the Prompt Engineer Course by DeepLearning.AI\n",
    "\n",
    "## Setup\n",
    "#### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c382975",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key  = 'your_OPENAI_API-keyword'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3acefa8-f8f1-4ef8-932e-9bcefa142666",
   "metadata": {},
   "source": [
    "\n",
    "We will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7dff174",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"] #there is better call?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b32ac4",
   "metadata": {},
   "source": [
    "**Note:** This notebook use OpenAI library version `0.27.0`. \n",
    "\n",
    "In order to use the OpenAI library version `1.0.0`, here is the code that you would use instead for the `get_completion` function:\n",
    "\n",
    "```python\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62298e-2181-4e73-bb40-77e20c655231",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "- **Principle 1: Write clear and specific instructions**\n",
    "- **Principle 2: Give the model time to “think”**\n",
    "\n",
    "### Tactics\n",
    "\n",
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acaecef4",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "eng_messages = []\n",
    "with open('engage_messages_2024.csv', mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    eng_messages = [item for sublist in csv_reader for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce125d5",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batches = [eng_messages[i:i + batch_size] for i in range(0, len(eng_messages), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03e17b8e",
   "metadata": {
    "height": 999
   },
   "outputs": [],
   "source": [
    "#I have added more contextual information about what it means to have a bad message and it understands. \n",
    "\n",
    "responses = {} \n",
    "import ast\n",
    "\n",
    "def qualitative ():\n",
    "    for batch in batches[:2]: # when they are a lot of messages, it may rushed to a conclusion\n",
    "        # without thinking. consider analyzing a sert of batches and then gather the info extracted \n",
    "        # for each set\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "        In the variable delimited by triple backticks, you find a list \\\n",
    "        of messages. These are messages from educators explaining how space inspires them \\\n",
    "        in their educator role.\\\n",
    "        I will give you a list of tasks that you have to perform in the order I am writing them to you\\\n",
    "        You can see that all task are delimited by single quotes\\\n",
    "\n",
    "        Your task in the following order is:\\\n",
    "        '0)' print messages that express bad feelings towared ESA conference \\\n",
    "        '1)' collect and print 2 main topics in all messages \\\n",
    "        '2)' collect and print 2 predominant emotions in all messages \\\n",
    "        '3)' print a summary of all the messages in 250 words \\\n",
    "        '4)' print an example of one literal real message that you find informative of the summary \\\n",
    "         Format your response as a JSON object with \"bad_messages\", \"Topics\" \n",
    ", \"Emotions\", \"Summary\" and \"Message\" as the keys           \n",
    "\n",
    "        ```{batch}```\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        response = get_completion(prompt)\n",
    "        #print('batch=', batch, response, \"\\n\\n\") uncomment for printing the messges\n",
    "        print(response, \"\\n\\n\")\n",
    "        response = ast.literal_eval(response)\n",
    "        for key, value in response.items():\n",
    "            if key in responses:\n",
    "                responses[key] += value  # Add the values of the matching keys\n",
    "            else:\n",
    "                responses[key] = value  # Add new key-value pairs if not already in dict\n",
    "        \n",
    "        \n",
    "    return responses\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dd67f74",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bad_messages\": [],\n",
      "    \"Topics\": [\n",
      "        \"STEM education\",\n",
      "        \"Space exploration\"\n",
      "    ],\n",
      "    \"Emotions\": [\n",
      "        \"Fascination\",\n",
      "        \"Inspiration\"\n",
      "    ],\n",
      "    \"Summary\": \"Educators find space to be a captivating and inspiring topic that engages students in STEM education. They express fascination with the vastness of the universe, the potential for space exploration, and the impact of space technology on Earth. Space serves as a motivator for students to learn, expand their perspectives, and consider future careers in STEM fields. Educators aim to transmit their passion for space to students, encouraging curiosity, critical thinking, and problem-solving skills. They see space as a gateway to innovation, technological advancements, and addressing global challenges.\",\n",
      "    \"Message\": \"Space inspires me because it connects me with some of my students. At this moment I have two students with autisme and giftedness, they enjoy staying in the class during the breaks and learn about Space with me. I was moved by the fact that you gave us, as primary and secondary teachers, the feeling that we and our young students matter during a workshop. So gratefull for that! Having a daughter in civil engineering and a son in science makes Space a mutual interest which is a gift to have for me as a mother.\"\n",
      "} \n",
      "\n",
      "\n",
      "{\n",
      "    \"bad_messages\": [],\n",
      "    \"Topics\": [\n",
      "        \"Space exploration and technology\",\n",
      "        \"Inspiration and motivation for teaching\"\n",
      "    ],\n",
      "    \"Emotions\": [\n",
      "        \"Inspiration\",\n",
      "        \"Curiosity\"\n",
      "    ],\n",
      "    \"Summary\": \"Educators express their deep inspiration and curiosity towards space, highlighting how it motivates them to teach and share their passion with students. They mention the wonders of space, the mysteries it holds, and the endless opportunities for learning and exploration. Many educators share personal experiences of how space has fascinated them since childhood, driving their interest in subjects like math, physics, and astronomy. They emphasize the importance of incorporating space-related topics into education to engage students and spark their curiosity. Overall, the messages convey a sense of wonder and enthusiasm for space and its potential to inspire both educators and students.\",\n",
      "    \"Message\": \"I am fascinated by the fact, that we still have a lot to explore not only in the whole Universe, but even within our solar system. I'm interested in the history of astronautics as well and how dangerous it used to be. Space is a subject barely taught in school and it is usually a lot of fun (for both sides) to discuss such things with children in our science center.\"\n",
      "} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = qualitative()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47e340",
   "metadata": {},
   "source": [
    "#### Tactic 2: Ask for a structured output\n",
    "- JSON, HTML\n",
    "\n",
    "##### I am outputting a Python dictionary, then saving it as a JSON file for reading/writing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e92a2655",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "responses\n",
    "# for saving the dictionary, I can use a JSON or directly save the dictionary in a python module, so to say \n",
    "#(save a file that contains the dict)\n",
    "# will use a json\n",
    "import json\n",
    "with open('space_engagement_online.json', 'w') as json_file:\n",
    "    json.dump(responses, json_file)\n",
    "\n",
    "#to load the JSON file through another python file\n",
    "#with opoen (space_engagement_online.json, 'r') as json_file:\n",
    "# loaded_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f78754",
   "metadata": {},
   "source": [
    "#### Tactic 3: Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba758fd",
   "metadata": {},
   "source": [
    "Add something like this inside your prompt\n",
    "\n",
    "'If the messages does not contain bad emotions, output a list with the only element\n",
    "\\\"No bad emotions.\\\"'\n",
    "\n",
    "## ATTENTION! LLMs have great difficulties when handling with negations. It may output that the messages contain bad emotions when these appear negated as it is in the case of : *I have experienced no bad emotions*\n",
    "\n",
    "### This is a common thing to look at when dealing with EHR (Elctronic Health Records).\n",
    "\n",
    "#### Here, I provide some references from the research literature:\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5866b8-d8c7-4e19-93db-401315f64954",
   "metadata": {},
   "source": [
    "#### Tactic 4: \"Few-shot\" prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce1540",
   "metadata": {
    "height": 251,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\ \n",
    "valley flows from a modest spring; the \\ \n",
    "grandest symphony originates from a single note; \\ \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7a8ee-1a2d-415d-8c10-500ecff24b10",
   "metadata": {},
   "source": [
    "### Principle 2: Give the model time to “think” \n",
    "\n",
    "#### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb1dcf-95f5-4ee1-8c25-8b2abd5f0f0d",
   "metadata": {},
   "source": [
    "#### Ask for output in a specified format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec80fdb-92db-48f6-8f1d-b03c26385bad",
   "metadata": {},
   "source": [
    "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion\n",
    "##### We can fix this by instructing the model to work out its own solution first and then compare to the solution provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573b758",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "\n",
    "### How can we track and solve hallucinations?\n",
    "\n",
    "First, what is an hallucination?\n",
    "\n",
    "```python\n",
    "f\"\"\"\n",
    "<user>: what is an hallucination of a LLM?\n",
    "\n",
    "<ChatGPT> said:\n",
    "\n",
    "In the context of Large Language Models (LLMs) like GPT, a hallucination refers to the phenomenon where the model generates information that is factually incorrect, nonsensical, or completely made up, even though the output may appear plausible or convincing.\n",
    "\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e18fb1",
   "metadata": {},
   "source": [
    "## How to reference the use of LLMs? [1]\n",
    "\n",
    "When prompted \"what is an hallucination of a LLM\" the ChatGPT-generated text indicated that \n",
    "\n",
    "in the context of Large Language Models (LLMs) like GPT, a hallucination refers to the phenomenon where the model generates information that is factually incorrect, nonsensical, or completely made up, even though the output may appear plausible or convincing.[*]\n",
    "\n",
    "[*]OpenAI (2024), ChatGPT (GPT-4)[Large Language Model]. Response to query made by ALba Morquecho-Delgado. 22/09/2024. https://chat.openai.com/chat\n",
    "\n",
    "[1] Hosseini, M., Resnik, D. B., & Holmes, K. (2023). The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts. Research Ethics, 19(4), 449-465. https://doi.org/10.1177/17470161231180449"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef0aa7-77e5-465e-a0ac-21c7156c9339",
   "metadata": {},
   "source": [
    "##### A note about the backslash\n",
    "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
    "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c1dcc-1cef-42f7-9291-fa1dfa9fcc1b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
